{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"55ea86a3-1012-41c9-b29e-25e9562c10e8","_cell_guid":"c2923b89-8760-41ab-b386-f37de0d935d2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-19T13:53:05.996726Z","iopub.execute_input":"2023-09-19T13:53:05.997039Z","iopub.status.idle":"2023-09-19T13:53:06.326974Z","shell.execute_reply.started":"2023-09-19T13:53:05.997015Z","shell.execute_reply":"2023-09-19T13:53:06.326046Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport nltk\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm","metadata":{"_uuid":"3e0d23e8-52cd-4e00-9004-1cf71f61ad00","_cell_guid":"b08fac51-2d58-4501-9266-e78f4beea8cb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-19T13:53:06.328452Z","iopub.execute_input":"2023-09-19T13:53:06.328904Z","iopub.status.idle":"2023-09-19T13:53:08.439178Z","shell.execute_reply.started":"2023-09-19T13:53:06.328878Z","shell.execute_reply":"2023-09-19T13:53:08.438005Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\n    nltk.data.find('stopwords.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    nltk.download('stopwords', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:53:08.440515Z","iopub.execute_input":"2023-09-19T13:53:08.440789Z","iopub.status.idle":"2023-09-19T13:53:08.925833Z","shell.execute_reply.started":"2023-09-19T13:53:08.440769Z","shell.execute_reply":"2023-09-19T13:53:08.924235Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data] Downloading package stopwords to /kaggle/working/...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nArchive:  /kaggle/working/corpora/wordnet.zip\n   creating: /kaggle/working/corpora/wordnet/\n  inflating: /kaggle/working/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/corpora/wordnet/README  \n  inflating: /kaggle/working/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_text(text, stop_words, lemmatizer):\n    if pd.isna(text):\n        return ''\n\n    # Cleaning operations\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n\n    words = word_tokenize(text.lower())\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n\n\n    return ' '.join(words)\n\n\n\ndef preprocess_data(input_file, output_file, chunk_size=1000):\n    stop_words = set(stopwords.words('english'))\n    custom_stopwords = [\n        \"ability\", \"able\", \"absolute\", \"absolutely\", \"account\", \"accurate\", \"achieve\", \"address\",\n        \"allowing\", \"also\", \"analyze\", \"analyzes\", \"answer\", \"application\", \"approach\",\n        \"around\", \"article\", \"aspect\", \"audience\", \"author\", \"available\", \"based\", \"begin\", \"best\", \"better\",\n        \"beyond\", \"bound\", \"brief\", \"called\", \"capable\", \"capture\", \"carefully\", \"case\", \"certain\", \"challenging\",\n        \"compare\", \"complex\", \"component\", \"comprehensive\", \"concept\", \"conceptual\",\n        \"conclusion\", \"condition\", \"conduct\", \"conjecture\", \"consider\", \"construct\", \"content\", \"context\", \"cost\",\n        \"cross\", \"crucial\", \"current\", \"demonstrate\", \"derive\", \"derived\", \"describe\",\n        \"described\", \"describes\", \"detailed\", \"determine\", \"developed\", \"different\", \"difficult\", \"directly\",\n        \"discourse\", \"discuss\", \"distinguish\", \"driven\", \"due\", \"effect\", \"effective\", \"efficient\", \"efficiently\",\n        \"eight\", \"element\", \"emphasis\", \"end\", \"enhanced\", \"evaluate\", \"even\", \"example\", \"experiment\", \"experimental\",\n        \"explain\", \"extensive\", \"family\", \"feature\", \"figure\", \"find\", \"fine\", \"finite\", \"finitely\", \"first\",\n        \"fit\", \"five\", \"found\", \"four\", \"framework\", \"function\", \"fundamental\", \"future\", \"general\", \"give\",\n        \"given\", \"good\", \"grained\", \"graph\", \"group\", \"handed\", \"high\", \"higher\", \"however\", \"illustrate\", \"impact\",\n        \"implement\", \"important\", \"include\", \"included\", \"integrate\", \"interest\", \"introduce\", \"introduced\", \"introduction\",\n        \"investigate\", \"issue\", \"iteration\", \"known\", \"large\", \"last\", \"leading\", \"left\", \"let\", \"long\", \"low\", \"lower\", \"make\",\n        \"many\", \"maximal\", \"may\", \"method\", \"methodology\", \"minimal\", \"model\", \"moreover\", \"multiple\",\n        \"necessary\", \"need\", \"needed\", \"new\", \"news\", \"next\", \"nine\", \"non\", \"note\", \"novel\", \"number\", \"numerical\",\n        \"objective\", \"observables\", \"observation\", 'obtained', \"often\", \"one\", \"open\", \"operator\", \"optimal\", \"order\",\n        \"outline\",\n        \"outlines\", \"output\", \"paper\", \"papr\", \"parameter\", \"part\", \"particular\", \"perform\",\n        \"performance\", \"performed\", \"performing\", \"performance\", \"phase\", \"point\", \"possible\",\n        \"potential\", \"pre\", \"precisely\", \"present\", \"previous\", \"principle\", \"problem\", \"process\", \"prof\",\n        \"proof\", \"proper\", \"property\", \"propose\", \"proposed\", \"proposes\", \"prove\", \"provide\", \"provided\",\n        \"publicly\", \"publish\", \"purpose\", \"quality\", \"question\", \"range\", \"real\", \"recent\", \"recently\",\n        \"recommendation\", \"related\", \"reliable\", \"representation\", \"require\", \"research\", \"result\", \"rev\", \"review\",\n        \"right\", \"rigorous\", \"role\", \"scale\", \"scenario\", \"second\", \"section\", \"selection\", \"series\", \"serious\", \"set\", \"setting\",\n        \"seven\", \"show\", \"significant\", \"significantly\", \"simulation\", \"single\", \"six\", \"solution\", \"state\",\n        \"strongly\", \"structure\", \"studied\", \"study\", \"sufficient\", \"suggestion\", \"sum\", \"synthesize\", \"system\",\n        \"table\", \"take\", \"taken\", \"task\", \"technique\", \"ten\", \"term\", \"theorem\", \"theory\", \"third\",\n        \"though\", \"three\", \"time\", \"topic\", \"two\", \"type\", \"upper\", \"use\", \"used\", \"using\", \"utilize\", \"valid\",\n        \"value\", \"variable\", \"variety\", \"various\", \"via\", \"view\", \"way\", \"well\", \"whether\", \"wide\", \"widely\", \"within\",\n        \"without\", \"work\", \"world\", \"written\", \"year\", \"zero\", \"zeroth\"]\n    stop_words.update(custom_stopwords)\n    lemmatizer = WordNetLemmatizer()\n\n    with open(input_file, 'r') as f:\n        total_rows = sum(1 for _ in f)\n\n    output_exists = os.path.isfile(output_file)\n\n    with tqdm(total=total_rows) as pbar:\n        for chunk in pd.read_json(input_file, lines=True, chunksize=chunk_size):\n            # drop duplicates\n            chunk = chunk[['title', 'abstract', 'categories', 'update_date']].drop_duplicates(\n                subset=['title', 'abstract'])\n            # preprocess text \n            chunk['title'] = chunk['title'].apply(preprocess_text, args=(stop_words, lemmatizer))\n            chunk['abstract'] = chunk['abstract'].apply(preprocess_text, args=(stop_words, lemmatizer))\n            # combine title and abstract into one column and drop original columns\n            chunk['text'] = chunk['title'] + ' ' + chunk['abstract']\n            chunk = chunk.drop(['title', 'abstract'], axis=1)\n            chunk = chunk[chunk['text'] != '']\n            # split categories into list of categories and save number of categories in a separate column and drop original column\n            chunk['categories_list'] = chunk['categories'].str.split()\n            chunk['num_categories'] = chunk['categories_list'].apply(len)\n            chunk = chunk.drop(['categories'], axis=1)\n            # convert update_date to datetime\n            chunk['update_date'] = pd.to_datetime(chunk['update_date'])\n            # save to csv\n            if not output_exists:\n                chunk.to_csv(output_file, mode='w', index=False)\n                output_exists = True\n            else:\n                chunk.to_csv(output_file, mode='a', index=False, header=False)\n\n            pbar.update(chunk_size)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:53:08.927773Z","iopub.execute_input":"2023-09-19T13:53:08.928129Z","iopub.status.idle":"2023-09-19T13:53:08.948601Z","shell.execute_reply.started":"2023-09-19T13:53:08.928105Z","shell.execute_reply":"2023-09-19T13:53:08.947418Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"raw_path = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\nprocessed_path = '/kaggle/working/data_preprocessed.csv'\npreprocess_data(raw_path, processed_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T13:53:08.949792Z","iopub.execute_input":"2023-09-19T13:53:08.950057Z","iopub.status.idle":"2023-09-19T14:38:16.872683Z","shell.execute_reply.started":"2023-09-19T13:53:08.950031Z","shell.execute_reply":"2023-09-19T14:38:16.871368Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2327000it [44:21, 874.32it/s]                              \n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom umap import UMAP","metadata":{"_uuid":"e5a3ab97-deac-4631-aa17-84d906a9551a","_cell_guid":"9b4d4c88-953d-4ee5-866c-9014724b00fc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-19T14:53:06.162984Z","iopub.execute_input":"2023-09-19T14:53:06.163730Z","iopub.status.idle":"2023-09-19T14:53:26.303316Z","shell.execute_reply.started":"2023-09-19T14:53:06.163698Z","shell.execute_reply":"2023-09-19T14:53:26.301587Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n/opt/conda/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n/opt/conda/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n/opt/conda/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n","output_type":"stream"}]},{"cell_type":"code","source":"input_file = '/kaggle/working/data_preprocessed.csv'\ndf = pd.read_csv(input_file)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:53:26.305804Z","iopub.execute_input":"2023-09-19T14:53:26.306470Z","iopub.status.idle":"2023-09-19T14:53:46.302453Z","shell.execute_reply.started":"2023-09-19T14:53:26.306444Z","shell.execute_reply":"2023-09-19T14:53:46.301345Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  update_date                                               text  \\\n0  2008-11-26  calculation prompt diphoton production section...   \n1  2008-12-13  sparsity certifying decomposition algorithm el...   \n2  2008-01-13  evolution earth moon dark matter field fluid e...   \n3  2007-05-23  determinant stirling cycle number count unlabe...   \n4  2013-10-15  dyadic lambda alpha lambda alpha compute lambd...   \n\n          categories_list  num_categories  \n0              ['hep-ph']               1  \n1    ['math.CO', 'cs.CG']               2  \n2      ['physics.gen-ph']               1  \n3             ['math.CO']               1  \n4  ['math.CA', 'math.FA']               2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>update_date</th>\n      <th>text</th>\n      <th>categories_list</th>\n      <th>num_categories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2008-11-26</td>\n      <td>calculation prompt diphoton production section...</td>\n      <td>['hep-ph']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2008-12-13</td>\n      <td>sparsity certifying decomposition algorithm el...</td>\n      <td>['math.CO', 'cs.CG']</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2008-01-13</td>\n      <td>evolution earth moon dark matter field fluid e...</td>\n      <td>['physics.gen-ph']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2007-05-23</td>\n      <td>determinant stirling cycle number count unlabe...</td>\n      <td>['math.CO']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2013-10-15</td>\n      <td>dyadic lambda alpha lambda alpha compute lambd...</td>\n      <td>['math.CA', 'math.FA']</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def vectorize_text(df):\n    # Instantiate TfidfVectorizer\n    vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n    X = vectorizer.fit_transform(df['text'])\n    return X, vectorizer\n\ntfidf_matrix, vectorizer = vectorize_text(df)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T14:53:46.303754Z","iopub.execute_input":"2023-09-19T14:53:46.304144Z","iopub.status.idle":"2023-09-19T15:04:12.941810Z","shell.execute_reply.started":"2023-09-19T14:53:46.304121Z","shell.execute_reply":"2023-09-19T15:04:12.940504Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Data Scaling\nscaler = StandardScaler(with_mean=False)\nX_scaled = scaler.fit_transform(tfidf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:04:12.944347Z","iopub.execute_input":"2023-09-19T15:04:12.944663Z","iopub.status.idle":"2023-09-19T15:04:14.604078Z","shell.execute_reply.started":"2023-09-19T15:04:12.944640Z","shell.execute_reply":"2023-09-19T15:04:14.602701Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Dimensionality reduction using LSA\ndef reduce_dimensions_lsa(X, n_components=150):\n    lsa = TruncatedSVD(n_components=n_components)\n    X_lsa = lsa.fit_transform(X)\n    return X_lsa\n\nX_lsa = reduce_dimensions_lsa(X_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:12:23.905149Z","iopub.execute_input":"2023-09-19T15:12:23.905995Z","iopub.status.idle":"2023-09-19T15:14:52.388223Z","shell.execute_reply.started":"2023-09-19T15:12:23.905966Z","shell.execute_reply":"2023-09-19T15:14:52.386537Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Taking a random sample of 20% of the data\nsample_indices = np.random.choice(X_lsa.shape[0], int(0.2 * X_lsa.shape[0]), replace=False)\nX_sample = X_lsa[sample_indices]\n\n# calculate the sum of squared distances for several k\nfig = plt.figure(figsize=(15,10))\nwcss={}\nfor i in range(2,16):\n    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42).fit(X_sample)\n    wcss[i] = kmeans.inertia_\n\n# Plot the results\nplt.plot(list(wcss.keys()),list(wcss.values()))\nplt.xlabel('Values for K')\nplt.ylabel('Sum of Squared Distances')\nplt.title('Elbow Method')\nplt.xticks(list(wcss.keys()))\nplt.grid(True)\nplt.savefig('../images/elbow.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:14:52.390332Z","iopub.execute_input":"2023-09-19T15:14:52.390702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_optimal_clusters(X):\n    silhouette_scores = []\n    davies_bouldin_scores = []\n\n\n    # Taking a random sample of 1% of the data\n    sample_indices = np.random.choice(X.shape[0], int(0.1 * X.shape[0]), replace=False)\n    X_sample = X[sample_indices]\n\n    # Define a range of clusters to test\n    cluster_range = range(2, 21)\n\n    for i in cluster_range:\n        kmeans = KMeans(n_clusters=i, n_init=10, random_state=0)\n        kmeans.fit(X_sample)\n\n        # Calculate silhouette score\n        silhouette_avg = silhouette_score(X_sample, kmeans.labels_)\n        silhouette_scores.append(silhouette_avg)\n\n\n        # Calculate Davies-Bouldin index\n        davies_bouldin_avg = davies_bouldin_score(X_sample, kmeans.labels_)\n        davies_bouldin_scores.append(davies_bouldin_avg)\n\n        # Print the score for each cluster\n        print(\"For n_clusters = {}, silhouette score is {})\".format(i, silhouette_avg))\n        print(\"For n_clusters = {}, Davies-Bouldin score is {})\".format(i, davies_bouldin_avg))\n\n\n    # Plot the scores \n    plt.figure(figsize=(15, 5))\n\n    # Plot Silhouette Score\n    plt.subplot(1, 2, 1)\n    plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='-', color='g')\n    plt.xticks(cluster_range)\n    plt.title('Silhouette Score')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Score')\n    plt.grid(True)\n    plt.savefig('../images/silhouette.png')\n\n    # Plot Davies-Bouldin Score\n    plt.subplot(1, 2, 2)\n    plt.plot(cluster_range, davies_bouldin_scores, marker='o', linestyle='-', color='r')\n    plt.xticks(cluster_range)\n    plt.title('Davies-Bouldin Score')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Score')\n    plt.grid(True)\n    plt.savefig('../images/davies.png')\n    \n    plt.show()\n\n# Call the function to determine optimal clusters\ndetermine_optimal_clusters(X_lsa)","metadata":{},"execution_count":null,"outputs":[]}]}