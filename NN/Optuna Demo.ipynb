{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-25T06:13:43.635003Z",
     "start_time": "2024-12-25T06:13:43.489300Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T06:13:45.404732Z",
     "start_time": "2024-12-25T06:13:45.401717Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install optuna",
   "id": "ca4d8d21196caf46",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T06:13:45.801794Z",
     "start_time": "2024-12-25T06:13:45.793630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Step 1: Create a simple MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Step 2: Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Step 2.1: Generate synthetic data (replace with your actual dataset)\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "    # Step 2.2: Split the data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Step 2.3: Define the model\n",
    "    input_size = X.shape[1]\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256)  # Hyperparameter to tune\n",
    "    output_size = 2  # Binary classification\n",
    "\n",
    "    model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Step 2.4: Define optimizer and loss function\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Step 2.5: Train the model\n",
    "    epochs = 5\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Step 2.6: Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, targets = batch\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Step 2.7: Calculate validation accuracy\n",
    "        accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Step 2.8: Early stopping based on validation accuracy\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Step 3: Run the Optuna optimization\n",
    "def optimize():\n",
    "    # Step 3.1: Create a study and specify the optimization goal\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    # Step 3.2: Optimize the objective function\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    # Step 3.3: Print the best parameters and best value\n",
    "    print(f\"Best Trial: {study.best_trial.params}\")\n",
    "    print(f\"Best Accuracy: {study.best_value}\")\n"
   ],
   "id": "9c264b1c02fbfed4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T06:13:49.816895Z",
     "start_time": "2024-12-25T06:13:46.686155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Execute the optimization\n",
    "optimize()\n",
    "\n",
    "\"\"\"\n",
    "Optuna优化：\n",
    "\n",
    "创建 study 对象并指定优化目标为最大化验证准确率（direction=\"maximize\"）。\n",
    "使用 study.optimize 方法开始优化过程，进行多次超参数试验。\n",
    "最后，输出最佳的超参数组合和对应的验证准确率\n",
    "\"\"\""
   ],
   "id": "5b994716a09c7dec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 14:13:46,687] A new study created in memory with name: no-name-8f012565-fb7c-43d6-a99f-16a3312a750e\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:47,598] Trial 0 finished with value: 0.795 and parameters: {'hidden_size': 165, 'learning_rate': 2.2934478632163862e-05}. Best is trial 0 with value: 0.795.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:47,725] Trial 1 finished with value: 0.64 and parameters: {'hidden_size': 160, 'learning_rate': 1.2778131005718543e-05}. Best is trial 0 with value: 0.795.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:47,860] Trial 2 finished with value: 0.79 and parameters: {'hidden_size': 144, 'learning_rate': 2.2437153839122087e-05}. Best is trial 0 with value: 0.795.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:47,998] Trial 3 finished with value: 0.865 and parameters: {'hidden_size': 129, 'learning_rate': 0.0011049634514120614}. Best is trial 3 with value: 0.865.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,135] Trial 4 finished with value: 0.85 and parameters: {'hidden_size': 105, 'learning_rate': 0.0002290936306480502}. Best is trial 3 with value: 0.865.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,255] Trial 5 finished with value: 0.855 and parameters: {'hidden_size': 86, 'learning_rate': 0.0011247989959991974}. Best is trial 3 with value: 0.865.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,385] Trial 6 finished with value: 0.84 and parameters: {'hidden_size': 85, 'learning_rate': 0.004830107255267293}. Best is trial 3 with value: 0.865.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,599] Trial 7 finished with value: 0.85 and parameters: {'hidden_size': 225, 'learning_rate': 0.00398192679961897}. Best is trial 3 with value: 0.865.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,631] Trial 8 pruned. \n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,667] Trial 9 pruned. \n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,697] Trial 10 pruned. \n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,727] Trial 11 pruned. \n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:48,909] Trial 12 finished with value: 0.865 and parameters: {'hidden_size': 212, 'learning_rate': 0.0012363775027945474}. Best is trial 3 with value: 0.865.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:49,090] Trial 13 finished with value: 0.875 and parameters: {'hidden_size': 250, 'learning_rate': 0.0022569882664130085}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:49,284] Trial 14 finished with value: 0.835 and parameters: {'hidden_size': 244, 'learning_rate': 0.00804388845157071}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:49,455] Trial 15 finished with value: 0.87 and parameters: {'hidden_size': 178, 'learning_rate': 0.00037708818660021724}. Best is trial 13 with value: 0.875.\n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:49,507] Trial 16 pruned. \n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:49,554] Trial 17 pruned. \n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:49,610] Trial 18 pruned. \n",
      "C:\\Users\\Citrus柚子\\AppData\\Local\\Temp\\ipykernel_21272\\3442130136.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # Learning rate range\n",
      "[I 2024-12-25 14:13:49,812] Trial 19 finished with value: 0.85 and parameters: {'hidden_size': 233, 'learning_rate': 0.0004966605729498082}. Best is trial 13 with value: 0.875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial: {'hidden_size': 250, 'learning_rate': 0.0022569882664130085}\n",
      "Best Accuracy: 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOptuna优化：\\n\\n创建 study 对象并指定优化目标为最大化验证准确率（direction=\"maximize\"）。\\n使用 study.optimize 方法开始优化过程，进行多次超参数试验。\\n最后，输出最佳的超参数组合和对应的验证准确率\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0ecd193a36b1d09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
